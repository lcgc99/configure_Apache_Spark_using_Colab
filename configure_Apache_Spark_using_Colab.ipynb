{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfRO-CZ6-Jbq"
      },
      "source": [
        "First, let's install _JAVA, Spark,_ and the module _findspark_. The last one is used when you work without Anaconda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Wl4bSr-DaJ"
      },
      "source": [
        "# Install JAVA\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "\n",
        "# Install for use with pypthon\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RCUJ8JD-Tz_"
      },
      "source": [
        "Then, we need to define the Environment Variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srw9fp2q-ZlV"
      },
      "source": [
        "import os #\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRfFU6E-fn9"
      },
      "source": [
        "Now, let's configure Spark UI. For that, we need to install ngrok and import some modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMQOL9ph-g8D"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Install ngrok\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Configure Spark UI\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WjpY4KU-o4f"
      },
      "source": [
        "To open Spark UI, you need to execute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX0zcRED-npx"
      },
      "source": [
        "# Create a URL through you can access the Spark UI\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heb2wQsP-zmp"
      },
      "source": [
        "And then, wait 10 seconds to run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dtuB5OR-0lH"
      },
      "source": [
        "# Then wait for 10s to access the URL\n",
        "!curl -s http://localhost:4040/api/tunnels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s-ybnKE-6dE"
      },
      "source": [
        "\n",
        "If you don't have any context active, Spark UI will show a failed connection. The context created before is only to configure Spark UI. Because of that, we stop it.\n",
        "\n",
        "\n",
        "Finally, I'm going to show you how to attach files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC1DymVO_AoS"
      },
      "source": [
        "# Add the documents source from which the necessary files will be taken\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UrnjHiV_Def"
      },
      "source": [
        "Here, you need to give permissions to Google Colab. When you do it, Google will give you a password. Left to define our path, in which we have our files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ylSgOX_F1H"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/fundamentos-de-Spark/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsEFdBAE_KJ4"
      },
      "source": [
        "\n",
        "You need to fit your own path.\n",
        "\n",
        "That's all! Now you can work with Apache Spark."
      ]
    }
  ]
}
